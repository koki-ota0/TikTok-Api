{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57bd520-3000-4e53-b2a1-9d06b68b175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "# 日本語フォントのパスを指定（使用するフォントに応じてパスを変更してください）\n",
    "font_path = \"C:\\Windows\\Fonts\\msyh.ttc\"\n",
    "# フォントマネージャにフォントを登録\n",
    "font_prop = fm.FontProperties(fname=font_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641df7fd-6c2e-486c-b1cd-4a19691d8e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ファイルの結合\n",
    "channel = \"atsuhiko_nakata\"\n",
    "\n",
    "file_paths = []\n",
    "for i in range(15):\n",
    "    file_paths.append(f\"data/{channel}/{channel}_comments{i}.json\")\n",
    "\n",
    "# 空のリストを作成して、各JSONファイルの内容を読み込み、結合する\n",
    "merged_data = myf.merge_json(file_paths)\n",
    "\n",
    "# 結合したデータを新しいJSONファイルに保存\n",
    "merged_file_path = f\"data/{channel}/{channel}_comments.json\"\n",
    "with open(merged_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(merged_data, file, ensure_ascii=False)\n",
    "print(len(merged_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa6053-b796-40e0-9adf-31dbbdb8b89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel = \"jinguji_ai_\"\n",
    "file_path = f\"data/{channel}/{channel}\"\n",
    "video_path = f\"{file_path}_videos.json\"\n",
    "\n",
    "df_videos = myf.read_json(video_path)\n",
    "df_videos['viewCount'] = pd.to_numeric(df_videos['viewCount'], errors='coerce')\n",
    "df_videos['likeCount'] = pd.to_numeric(df_videos['likeCount'], errors='coerce')\n",
    "df_videos['commentCount'] = pd.to_numeric(df_videos['commentCount'], errors='coerce')\n",
    "df_videos['publishedDate'] = df_videos['publishedDate'].apply(lambda timestamp: datetime.datetime.fromtimestamp(timestamp))\n",
    "\n",
    "# trailer_df = video_df[video_df['title'].str.contains('Trailer')]\n",
    "comment_path = f\"{file_path}_comments.json\"\n",
    "df_comments = myf.read_json(comment_path)\n",
    "df_comments['likeCount'] = pd.to_numeric(df_comments['likeCount'], errors='coerce')\n",
    "df_comments['replyCount'] = pd.to_numeric(df_comments['replyCount'], errors='coerce')\n",
    "df_comments['commentDate'] = df_comments['commentDate'].apply(lambda timestamp: datetime.datetime.fromtimestamp(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49babf2a-3b19-4097-a37f-629d5a3f9379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 動画情報とコメント情報を結合\n",
    "merged_df = pd.merge(df_comments, df_videos, on='videoId')\n",
    "\n",
    "# 動画が投稿されてからの時間差を計算\n",
    "merged_df['time_since_published'] = (merged_df['commentDate'] - merged_df['publishedDate']).dt.total_seconds() / 3600  # 時間単位\n",
    "merged_df['time_since_published'] = merged_df['time_since_published'].astype(int)\n",
    "\n",
    "# 時間ごとにコメント数を集計\n",
    "comment_count_by_time = merged_df.groupby('time_since_published').size()\n",
    "\n",
    "# グラフを作成して表示\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(comment_count_by_time.index, comment_count_by_time.values, marker='o')\n",
    "plt.xlabel('Time Since Published (hours)')\n",
    "plt.ylabel('Comment Count')\n",
    "plt.title('Comment Count by Time Since Published')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a372e58-8d9d-4e2d-8ac3-809a9f02719e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 日付ごとにコメント数を集計\n",
    "comment_count_by_date = df_comments.groupby(comments['commentDate'].dt.date).size()\n",
    "\n",
    "# グラフを作成して表示\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(comment_count_by_date.index, comment_count_by_date.values, marker='o')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Comment Count')\n",
    "plt.title('Comment Count by Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e02c5a-496b-48db-84fc-0ddb28aca72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 動画情報とコメント情報を結合\n",
    "merged_df = pd.merge(df_comments, df_videos, on='videoId')\n",
    "\n",
    "# 動画が投稿されてからの時間差を計算\n",
    "merged_df['time_since_published'] = (merged_df['commentDate'] - merged_df['publishedDate']).dt.total_seconds() / 3600  # 時間単位\n",
    "\n",
    "# 'AI'という文字列が含まれているかどうかを判定する列を追加\n",
    "merged_df['contains_ai'] = merged_df['text'].str.contains('AI', case=False)\n",
    "\n",
    "# 'AI'を含む動画のデータ\n",
    "ai_videos = merged_df[merged_df['contains_ai']]\n",
    "# 'AI'を含まない動画のデータ\n",
    "non_ai_videos = merged_df[~merged_df['contains_ai']]\n",
    "\n",
    "# 動画ごとにコメント数を集計（AIを含む動画）\n",
    "ai_comment_count_by_time = ai_videos.groupby('time_since_published').size().cumsum()\n",
    "# 動画ごとにコメント数を集計（AIを含まない動画）\n",
    "non_ai_comment_count_by_time = non_ai_videos.groupby('time_since_published').size().cumsum()\n",
    "\n",
    "# グラフを作成して表示\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ai_comment_count_by_time.index, ai_comment_count_by_time.values, marker='o', label='AI Videos')\n",
    "plt.plot(non_ai_comment_count_by_time.index, non_ai_comment_count_by_time.values, marker='x', label='Non-AI Videos')\n",
    "plt.xlabel('Time Since Published (hours)')\n",
    "plt.ylabel('Cumulative Comment Count')\n",
    "plt.title('Cumulative Comment Count by Time Since Published')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6fa8c-ab83-4cf1-b797-37284ba26d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 動画情報とコメント情報を結合\n",
    "merged_df = pd.merge(df_comments, df_videos, on='videoId')\n",
    "\n",
    "# 動画が投稿されてからの時間差を計算\n",
    "merged_df['time_since_published'] = (merged_df['commentDate'] - merged_df['publishedDate']).dt.total_seconds() / 3600  # 時間単位\n",
    "merged_df['time_since_published'] = merged_df['time_since_published'].astype(int)\n",
    "\n",
    "# 'AI'という文字列が含まれているかどうかを判定する列を追加\n",
    "merged_df['contains_ai'] = merged_df['text'].str.contains('AI', case=False)\n",
    "\n",
    "# 動画ごとにコメント数を集計（AIを含む動画）\n",
    "ai_comment_count_by_video = ai_videos.groupby(['videoId', 'time_since_published']).size().reset_index(name='comment_count')\n",
    "ai_comment_count_by_video['cumulative_comment_count'] = ai_comment_count_by_video.groupby('videoId')['comment_count'].cumsum()\n",
    "\n",
    "# 動画ごとにコメント数を集計（AIを含まない動画）\n",
    "non_ai_comment_count_by_video = non_ai_videos.groupby(['videoId', 'time_since_published']).size().reset_index(name='comment_count')\n",
    "non_ai_comment_count_by_video['cumulative_comment_count'] = non_ai_comment_count_by_video.groupby('videoId')['comment_count'].cumsum()\n",
    "\n",
    "mered_df = merged_df.sort_values('publishedDate')\n",
    "\n",
    "# 動画ごとに累積折れ線グラフを作成して表示\n",
    "unique_video_ids = merged_df['videoId'].unique()\n",
    "for video_id in unique_video_ids:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # AIを含む動画の累積折れ線グラフ\n",
    "    ai_data = ai_comment_count_by_video[ai_comment_count_by_video['videoId'] == video_id]\n",
    "    plt.plot(ai_data['time_since_published'], ai_data['cumulative_comment_count'], marker='o', label='AI Videos')\n",
    "    \n",
    "    # AIを含まない動画の累積折れ線グラフ\n",
    "    non_ai_data = non_ai_comment_count_by_video[non_ai_comment_count_by_video['videoId'] == video_id]\n",
    "    plt.plot(non_ai_data['time_since_published'], non_ai_data['cumulative_comment_count'], marker='x', label='Non-AI Videos')\n",
    "    \n",
    "    plt.xlabel('Time Since Published (hours)')\n",
    "    plt.ylabel('Cumulative Comment Count')\n",
    "    plt.title(f'Cumulative Comment Count by Time Since Published for VideoID: {video_id}')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333dc531-2010-4ad8-aed3-f8edabf02611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 動画情報とコメント情報を結合\n",
    "merged_df = pd.merge(df_comments, df_videos, on='videoId')\n",
    "\n",
    "# 'AI'という文字列が含まれているかどうかを判定する列を追加\n",
    "merged_df['contains_ai'] = merged_df['text'].str.contains('AI', case=False)\n",
    "\n",
    "# 'AI'を含む動画のデータ\n",
    "ai_videos = merged_df[merged_df['contains_ai']]\n",
    "# 'AI'を含まない動画のデータ\n",
    "non_ai_videos = merged_df[~merged_df['contains_ai']]\n",
    "\n",
    "# 動画ごとにコメント数を集計（AIを含む動画）\n",
    "ai_comment_count_by_time = ai_videos.groupby('commentDate').size().cumsum()\n",
    "# 動画ごとにコメント数を集計（AIを含まない動画）\n",
    "non_ai_comment_count_by_time = non_ai_videos.groupby('commentDate').size().cumsum()\n",
    "\n",
    "# グラフを作成\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ユニークなpublishedDateの値を取得\n",
    "unique_published_dates = merged_df['publishedDate'].sort_values().unique()\n",
    "\n",
    "# グラフをプロット\n",
    "plt.plot(ai_comment_count_by_time.index, ai_comment_count_by_time.values, marker='o', label='AI Videos')\n",
    "plt.plot(non_ai_comment_count_by_time.index, non_ai_comment_count_by_time.values, marker='x', label='Non-AI Videos')\n",
    "plt.vlines(unique_published_dates, ymin=0, ymax=max(ai_comment_count_by_time.max(), non_ai_comment_count_by_time.max()), colors='gray', linestyle='dashed', alpha=0.5, label='publishedDate')\n",
    "\n",
    "\n",
    "plt.xlabel('Published Date')\n",
    "plt.ylabel('Cumulative Comment Count')\n",
    "plt.title('Cumulative Comment Count by Published Date')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd24a7-858d-461b-8d6d-e70fa8a5119e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 動画情報とコメント情報を結合\n",
    "merged_df = pd.merge(df_comments, df_videos, on='videoId')\n",
    "\n",
    "# 'AI'という文字列が含まれているかどうかを判定する列を追加\n",
    "merged_df['contains_ai'] = merged_df['text'].str.contains('AI', case=False)\n",
    "\n",
    "# 動画ごとにコメント数を集計\n",
    "video_comment_count = merged_df.groupby('videoId').size()\n",
    "\n",
    "# 動画ごとにAIを含むコメント数を集計\n",
    "ai_comment_count = merged_df[merged_df['contains_ai']].groupby('videoId').size()\n",
    "\n",
    "# 動画ごとに非AIコメント数を集計\n",
    "non_ai_comment_count = video_comment_count - ai_comment_count\n",
    "\n",
    "# グラフを作成して表示\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(video_comment_count.index, ai_comment_count, label='AI Comments', color='blue')\n",
    "plt.bar(video_comment_count.index, non_ai_comment_count, bottom=ai_comment_count, label='Non-AI Comments', color='orange')\n",
    "plt.xlabel('Video')\n",
    "plt.ylabel('Comment Count')\n",
    "plt.title('Stacked Comment Count by Video')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 動画ごとに比率を計算して新しいDataFrameを作成\n",
    "ratios_df = pd.DataFrame({\n",
    "    'VideoID': video_comment_count.index,\n",
    "    'TotalComments': video_comment_count.values,\n",
    "    'AIComments': ai_comment_count.values,\n",
    "})\n",
    "ratios_df['NonAIComments'] = ratios_df['TotalComments'] - ratios_df['AIComments']\n",
    "ratios_df['AICommentRatio'] = ratios_df['AIComments'] / ratios_df['TotalComments']\n",
    "ratios_df['NonAICommentRatio'] = ratios_df['NonAIComments'] / ratios_df['TotalComments']\n",
    "\n",
    "# グラフを作成して表示\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(ratios_df.index, ratios_df['AICommentRatio'], label='AI Comments', color='blue')\n",
    "plt.bar(ratios_df.index, ratios_df['NonAICommentRatio'], bottom=ratios_df['AICommentRatio'], label='Non-AI Comments', color='orange')\n",
    "plt.xlabel('Video')\n",
    "plt.ylabel('Comment Ratio')\n",
    "plt.title('AI Comment Ratio vs. Non-AI Comment Ratio')\n",
    "plt.xticks(ratios_df.index, ratios_df['VideoID'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017e487-9d20-4d6e-a5a3-2f225fd7c9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "statistics = myf.get_statistics(df_comments['text'],channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdde05-1faf-4bd9-a693-e46eff384731",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_percentages = myf.lang_dist(df_comments['txt'])\n",
    "\n",
    "# 言語判定結果を保存\n",
    "with open(f\"data/{channel}/{channel}_lang_dist.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lang_percentages, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea53b2-a755-41f8-a1b8-4e32b52d02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"data/{channel}/{channel}_lang_dist.json\"\n",
    "with open(file_path) as f:\n",
    "    lang_percentages = json.load(f)\n",
    "\n",
    "# ラベルと割合のデータを準備\n",
    "labels = list(lang_percentages.keys())\n",
    "sizes = list(lang_percentages.values())\n",
    "\n",
    "# 割合の大きい順にソート\n",
    "labels, sizes = zip(*sorted(zip(labels, sizes), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "labels_tmp = []\n",
    "sizes_tmp = []\n",
    "for label, size in zip(labels, sizes):\n",
    "    if size >= 0.001:\n",
    "        labels_tmp.append(label)\n",
    "        sizes_tmp.append(size)\n",
    "labels = labels_tmp\n",
    "sizes = sizes_tmp\n",
    "\n",
    "# 円グラフを描画\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "wedges, texts, autotexts = ax.pie(sizes, labels=labels, startangle=90, \n",
    "                                  counterclock=False, autopct='%1.1f%%', pctdistance=0.85)\n",
    "\n",
    "# ラベルの位置を外側に配置\n",
    "for text in texts:\n",
    "    text.set_horizontalalignment('center')\n",
    "    text.set_verticalalignment('center')\n",
    "\n",
    "# 割合の値を表示\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_horizontalalignment('center')\n",
    "    autotext.set_verticalalignment('center')\n",
    "\n",
    "# 凡例を作成し、外側に配置\n",
    "legend = ax.legend(wedges, labels, title='Langage', loc='center left', \n",
    "                   bbox_to_anchor=(1, 0.5), fontsize=30, prop=font_prop)\n",
    "\n",
    "# 凡例に値を表示\n",
    "for i, text in enumerate(legend.get_texts()):\n",
    "    value = sizes[i]*100\n",
    "    text.set_text(f'{text.get_text()} ({value:.2f}%)')\n",
    "\n",
    "# グラフを表示\n",
    "plt.show()\n",
    "\n",
    "# グラフを保存\n",
    "save_file_path = f\"data/{channel}/{channel}_lang_dist.png\"\n",
    "plt.savefig(save_file_path, bbox_inches='tight')\n",
    "print(\"グラフを保存しました:\", save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c522c5-1520-4753-ae02-b2f0d870651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 各videoIdのコメント数をカウントし、上位10個を選択\n",
    "top_video_ids = df_comments['videoId'].value_counts().head(10).index.tolist()\n",
    "\n",
    "# videoIdごとにtextをまとめる\n",
    "grouped_texts = df_comments.groupby('videoId')['text'].apply(' '.join)\n",
    "\n",
    "# tf-idf計算\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(grouped_texts)\n",
    "print(tfidf_matrix)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 上位のvideoIdごとのワードクラウドを作成\n",
    "for video_id, row in zip(grouped_texts.index, tfidf_matrix):\n",
    "    if video_id not in top_video_ids:\n",
    "        continue\n",
    "\n",
    "    tfidf_scores = dict(zip([feature_names[i] for i in row.indices], row.data))\n",
    "\n",
    "    wc = WordCloud(background_color='white').generate_from_frequencies(tfidf_scores)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'WordCloud for videoId: {video_id}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd6ea5-2d14-4d74-af2d-5975b7561280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from janome.tokenizer import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 各videoIdのコメントをまとめる\n",
    "grouped_texts = df_comments.groupby('videoId')['text'].apply(' '.join)\n",
    "\n",
    "# 日本語テキストを分かち書き\n",
    "def tokenize_japanese_text(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    return ' '.join([token.surface for token in tokens])\n",
    "\n",
    "grouped_texts_jp = grouped_texts.apply(tokenize_japanese_text)\n",
    "\n",
    "# tf-idf計算\n",
    "vectorizer = TfidfVectorizer(stop_words=None)  # 日本語ではストップワードの指定不要\n",
    "tfidf_matrix = vectorizer.fit_transform(grouped_texts_jp)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 上位のvideoIdごとのワードクラウドを作成\n",
    "for video_id, row in zip(grouped_texts_jp.index, tfidf_matrix):\n",
    "    if video_id not in top_video_ids:\n",
    "        continue\n",
    "\n",
    "    tfidf_scores = dict(zip([feature_names[i] for i in row.indices], row.data))\n",
    "\n",
    "    wc = WordCloud(background_color='white', font_path=font_path).generate_from_frequencies(tfidf_scores)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'WordCloud for videoId: {video_id}')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
